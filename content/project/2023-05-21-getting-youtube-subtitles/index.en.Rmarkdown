---
title: Getting YouTube subtitles
author: twg
date: '2023-05-21'
slug: getting-youtube-subtitles
categories:
  - scraping
  - youtube
tags:
  - scraping
  - youtube
subtitle: ''
summary: 'Writing a script to get YouTube subtitles and all other data about a video'
authors: []
lastmod: '2023-05-21T16:52:35-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup-knitr, include = FALSE}
knitr::opts_chunk$set(
  eval = TRUE, # Default: TRUE
  include = TRUE, # Default: TRUE
  echo = TRUE, # Default: TRUE
  width = getOption("width"),
  fig.align = "center",
  fig.width = 11.5, # Default: 7
  fig.height = 5.75)

pacman::p_load(tidyverse, here, glue, janitor)
```

YouTube subtitles are a trove of text data, but I've struggled with a good way of extracting and cleaning them. Now, thanks to a few utilities, I've built a nice R function that pulls and cleans subtitles as well as all comments and other data for each video. 

## Setup

The first thing we need to do is install a few utilities that will do the heavy lifting. First is [yt-dlp](https://github.com/yt-dlp/yt-dlp), which is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl). Installation instructions are on the github page (use the `pip` functionality). It's very well documented and very powerful. We're using it to download the subtitles and video data, but this can actually download the entire video if you want.  

Next we need to download [ffmpeg](https://ffmpeg.org/). There are a number of ways of installing, but the easiest is probably just using Homebrew ("brew install ffmpeg"). 

Last, we need ripgrep to help to clean the text after download ("brew install ripgrep).

## Downloading the subtitles

These utilities function from the command line, but we can wrap our zsh commands in R's `system()` function to run in R. To get the subtitles, all we need is a YouTube url and the below. 

```{r, eval = FALSE}
system("yt-dlp --sub-lang en --skip-download --write-auto-sub --sub-format vtt --write-info-json --write-comments 'https://www.youtube.com/watch?v=Lz33OZf4Yx0'")
```

This command is going to download the subtitles in a .vtt format along with a json file that contains all the video data. And because we've included the command `--write-comments` it will also pull all the user comments for that video and include them in the json file. 

The subtitles and json are downloaded to the working directory. Both will be saved with the formal title of the video. We'll change that when everything gets put into a function.

If we read in the subtitles in .vtt format, we can see it's not in a clean format. 

```{r, echo = FALSE}
readLines(here::here('static', "data", "dmb.en.vtt"), n = 15)
```

## Cleaning our subtitles

We're going to use ffmpeg to convert these subtitles into a bit of a better format. Again, we're going to wrap our command line in our `system()` function to run it in R. As the code above is currently written, yt-dlp saves the data using the formal title of the video. So in the below code, the file name needs to be used. 

```{r, eval = FALSE}
system("ffmpeg -i '[FILENAME].en.vtt' [FILENAME].srt")
```

ffmpeg has converted our .vtt format into .srt format, but we can see that it's still not the most readable and a lot of work would need to be done if we were going to do any text analysis. 

```{r, echo = FALSE}
readLines(here::here('static', "data", "dmb.srt"), n = 15)
```

So now we're going to use ripgrep to clean this and make it readable. 

```{r, eval = FALSE}
system("rg -v '^[[:digit:]]+$|^[[:digit:]]{2}:|^$', yt_data/ [FILENAME].srt | tr -d '\r' | rg -N '\\S' | uniq > [CLEAN_FILENAME].txt")
```

And now we have a clean text ready to read or use for further NLP analysis. 

```{r, echo = FALSE}
readLines(here::here('static', "data", "clean_dmb.txt"), n = 15)
```

## Wrapping it in a function

We have everything we need to put this into a usable function. By default, yt-dlp uses the formal title of the video for the filename, but we can change that with the `--output` command where we're going to pass through our own title as an argument.   

```{r, eval = FALSE}
youtube_scrape <- function(link, title, comments=TRUE) {
  if (comments == TRUE) {
    x <- glue::glue("yt-dlp --sub-lang en --skip-download --write-auto-sub --sub-format vtt --write-info-json --write-comments --output 'yt_data/{title}' '{link}'")
  } else {
    x <- glue::glue("yt-dlp --sub-lang en --skip-download --write-auto-sub --sub-format vtt --write-info-json --output 'yt_data/{title}' '{link}'")
  }
  system(x)
  y <- glue::glue("ffmpeg -i 'yt_data/{title}.en.vtt' yt_data/{title}.srt")
  system(y)
  # glue messes up this command for some reason; using paste instead #
  z <- paste0("rg -v '^[[:digit:]]+$|^[[:digit:]]{2}:|^$', yt_data/",title, ".srt | tr -d '\r' | rg -N '\\S' | uniq > yt_data/clean_",title, ".txt")
  system(z)
}
```

## YouTube data

The json file is also downloaded, and we can easily access that. Using `jsonlite` the returned data is a named list. 

```{r, eval = FALSE}
out <- jsonlite::fromJSON(glue::glue("yt_data/{title}.info.json"))
names(out)
```

```{r, echo = FALSE}
names(jsonlite::fromJSON(here::here('static', "data", "dmb.info.json")))
```

From here we can easily pull the comment text. 

```{r eval = FALSE}
comments <- out$comments |> 
  mutate(text = str_replace_all(text, "\n", " "),
         text = str_replace_all(text, "  ", " "),
         timestamp = lubridate::as_datetime(as.numeric(timestamp, tz = "America/Los_Angeles")))
  
```

```{r, echo = FALSE}
jsonlite::fromJSON(here::here('static', "data", "dmb.info.json"))$comments |> 
  mutate(text = str_replace_all(text, "\n", " "),
         text = str_replace_all(text, "  ", " "),
         timestamp = lubridate::as_datetime(as.numeric(timestamp, tz = "America/Los_Angeles"))) |> 
  select(text, timestamp, like_count, author) |> 
  tibble() |> head()
```

